<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Dipkamal Blogs</title>
  <link rel="stylesheet" href="../assets/wowchemy.0f229d4b7ebad1917a9a357cba2effab.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    body { font-family: Arial, sans-serif; font-size: 18px; }
    .container-xl { max-width: 1140px; margin: 0 auto; padding: 0 15px; }
    .col-lg-8 { width: 100%; max-width: 800px; margin: 0 auto; }
    h1, h2, h3, h4, h5, h6 { font-family: Arial, sans-serif; font-weight: bold; }
    h1 { font-size: 45px; font-weight: bold; }
    h2 { font-size: 36px; }
    h3 { font-size: 28px; }
    h4 { font-size: 24px; }
    p { text-align: justify; }
    .blue { color: rgb(72, 72, 193); }
    .red { color: red; }
    img { max-width: 100%; height: auto; }
    figcaption { text-align: center; font-style: italic; }
    
    /* Collapsible TOC styles */
    .collapsible {
      background-color: #f1f1f1;
      cursor: pointer;
      padding: 18px;
      width: 100%;
      border: none;
      text-align: left;
      outline: none;
      font-size: 15px;
    }
    .active, .collapsible:hover {
      background-color: #ddd;
    }
    .content {
      padding: 0 18px;
      display: none;
      overflow: hidden;
      background-color: #f9f9f9;
    }

    .toc-list {
      list-style-type: none;
      counter-reset: section;
      padding-left: 0;
    }
    .toc-list > li {
      counter-increment: section;
    }
    .toc-list > li::before {
      content: counter(section) ". ";
    }
    .toc-list > li > ol {
      list-style-type: none;
      counter-reset: subsection;
      padding-left: 20px;
    }
    .toc-list > li > ol > li {
      counter-increment: subsection;
    }
    .toc-list > li > ol > li::before {
      content: counter(section) "." counter(subsection) " ";
    }
  </style>
</head>

<body>
  <div>
    <script src="assets/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js"></script>
    <div class="page-header header--fixed">
      <header>
        <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
          <!-- make the navbar fit in the middle 8 columns out of 12. And make all the list items align to the right -->
          <div class="container-xl col-lg-8">
            <div class="d-none d-lg-inline-flex"><a class="navbar-brand" href="https://dipkamal.github.io/">Dipkamal Bhusal</a></div><button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false"
              aria-label="Toggle navigation">
              <span><i class="fas fa-bars"></i></span></button>
            <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class="navbar-brand"
                href="https://dipkamal.github.io/">Dipkamal Bhusal</a></div>
            <!-- <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">
              <ul class="navbar-nav d-md-inline-flex">
                <li class="nav-item"><a class="nav-link active" href="#about"
                    data-target="#about"><span>About me</span></a></li>
                <li class="nav-item"><a class="nav-link" href="#featured"
                    data-target="#featured"><span>Research</span></a></li>
                <li class="nav-item"><a class="nav-link" href="#blogs"
                    data-target="#blogs"><span>Blogs</span></a></li>
              </ul>
            </div> -->
          </div>
        </nav>
      </header>
    </div>
  <div class="container-xl">
    <div class="col-lg-8">
      <header>
        <h1>Understanding Attention and Transformer</h1>
        <p style="color: gray;">October 12, 2022</p>
      </header>
          
      <div class="text-center">
        <img src="../assets/images/blogs/transformer.png" alt="Transformer and attention" class="img-fluid" style="max-width: 100%; height: auto;">
        <!-- <p class="figure-caption">Figure 1: Animation illustrating the continuous morphing of distribution p₀ to pₜ.</p> -->
      </div>

      <section id="introduction">
        <p>
        Vanilla RNNs encode word embedding in linear fashion following the principle that nearby words affect the meaning of a sentence. So, the number of steps required for distant word pair interaction 
        grows linearly in \( O(sequence\_length) |). This introduces difficulty in learning long distance dependencies because of vanishing gradient problem over long sequence. Encoder of a vanilla RNN uses only 
        one hidden state to capture all information of the input sequence. This causes an information bottleneck. We should extract as much information as possible from our input sequence for decoder part.
</p>

<p> For example: in given picture, we want to translate an input sentence in English to Nepali target sentence "ma keta ho". For this type of translation task, one uses a sequence-to-sequence model, which 
consists of an encoder and a decoder, both of which are RNNs. In encoder, each word produces a hidden vector (called context vector) and output vector. The context vector is passed onto the next word (time). 
The final hidden state (context vector) is passed onto the decoder part, to translate the sentence. 
</p>

<img src="../assets/images/blogs/transformer/vanilla seq model.png" alt="vanilla Seq model" style="width:100%; display:block; margin:auto;">


<p>  To improve long-distance dependencies by solving bottleneck and vanishing gradient problems, attention was proposed. It is a mechanism by which at each time step of decoder, 
we use direct connection with encoder that allows us to focus on a particular part of the sentence. By connection, it means that instead of just passing one context vector, 
all hidden states are passed to the decoder. Attention also provides some interpretability by learning where a decoder is focusing on. Another issue with recurrent models 
is lack of parallelizability that inhibits training on very large dataset. Attention eases parallelization. Transformer, the most popular NLP architecture, uses attention mechanism.</p>


Now, lets talk about the transformer model and break it down. At the core of the Transformer model is the **self-attention mechanism**, which allows the model to weigh the importance of different words in a sequence. In this post, we break down the Transformer model in detail, including input embeddings, positional encodings, self-attention, multi-head attention, and inference strategies.

## 2. Input Embeddings and Tokenization
A Transformer model starts with **input embeddings**, which convert words or subwords into numerical representations. Since neural networks operate on numbers, text input must be tokenized and mapped to fixed-size vectors.

1. **Tokenization**: A sentence is first tokenized into words, subwords, or characters.
2. **Mapping to IDs**: Each token is assigned a unique numerical ID based on a fixed vocabulary.
3. **Embedding lookup**: Each token ID is mapped to a high-dimensional dense vector using a learned **embedding matrix**.

Mathematically, if a vocabulary has `V` tokens and the embedding dimension is `d_model`, then the embedding matrix `E` is of shape `(V, d_model)`. Given a token `t_i` with index `i` in the vocabulary, its embedding is retrieved as:

\[
\mathbf{e}_i = E[i]
\]

Initially, the embedding matrix is randomly initialized (e.g., using **Xavier** or **Gaussian initialization**) and is **learned through training** via backpropagation. Since embeddings capture semantic meaning, words with similar meanings tend to have similar vector representations.

---

## 3. Positional Encodings
Unlike RNNs, Transformers do not have inherent sequential processing. To incorporate word order, **positional encodings** are added to embeddings. These encodings help the model recognize which words appear earlier or later in a sequence.

### 3.1. Why Use Trigonometric Functions?
Positional encodings are **not learned** but instead computed once using sine and cosine functions. These functions provide a continuous pattern that allows the model to generalize to different sequence lengths.

For a position `pos` and embedding dimension `d_model`, the encoding is computed as:

\[
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\]

\[
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\]

where:
- `i` is the index of a dimension in the embedding vector.
- `10000^(2i/d_model)` ensures different frequency scales for different dimensions.

Since sine and cosine functions form a periodic pattern, they help the model recognize relative positions effectively. The positional encoding is **added element-wise** to the input embeddings before being passed into the encoder:

\[
\mathbf{x}_i = \mathbf{e}_i + PE_i
\]

where `x_i` is the final input to the Transformer.

---

## 4. Self-Attention Mechanism
The **self-attention mechanism** enables the model to relate different words in a sentence. Instead of processing tokens sequentially, self-attention computes dependencies between **all** words at once.

### 4.1. Query, Key, and Value Vectors
For each input token, three vectors are computed using learned weight matrices:
- **Query (Q)**: Represents the word asking for information.
- **Key (K)**: Represents the word providing information.
- **Value (V)**: Contains the actual information to be passed.

These are computed as:

\[
Q = X W_Q, \quad K = X W_K, \quad V = X W_V
\]

where:
- `X` is the input embedding matrix.
- `W_Q`, `W_K`, and `W_V` are learnable weight matrices.

### 4.2. Attention Score Computation
The attention score between two words is computed using the dot product of their **query** and **key** vectors:

\[
\text{score} = Q K^T
\]

To prevent excessively large values, the score is scaled by the square root of the embedding dimension `d_k`:

\[
\text{Attention}(Q, K, V) = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right) V
\]

- The **softmax** function ensures that attention scores sum to 1, making them interpretable as probabilities.
- This attention mechanism allows words to influence each other dynamically, depending on their learned relationships.

### 4.3. Masking
To prevent words from attending to future words during training, a **masking mechanism** sets certain values to `-∞` before applying softmax. This ensures that a word only considers past words during decoding.

---

## 5. Multi-Head Attention
Instead of using a single attention mechanism, **multi-head attention** uses multiple **parallel** attention heads. Each head focuses on different relationships between words.

For each head `i`, separate weight matrices `W_Q^i`, `W_K^i`, and `W_V^i` are used:

\[
\text{head}_i = \text{Attention}(QW_Q^i, KW_K^i, VW_V^i)
\]

The outputs of all heads are concatenated and projected using another matrix `W_O`:

\[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O
\]

This allows the model to capture multiple types of relationships in parallel.

---

## 6. Add & Norm Layers
After multi-head attention, the output is **added** back to the original input embedding (residual connection) and **normalized** using **layer normalization**:

\[
\text{Norm}(\mathbf{x}) = \frac{\mathbf{x} - \mu}{\sigma}
\]

where `μ` and `σ` are the mean and standard deviation of the input.

This helps stabilize training and prevents vanishing/exploding gradients.

---

## 7. The Decoder and Masked Attention
The decoder follows a similar structure as the encoder but with **masked multi-head attention** to ensure causality.

- The **first** self-attention layer in the decoder is **masked** to prevent words from attending to future words.
- The **second** attention layer (encoder-decoder attention) attends to the encoder outputs.
- The **final** fully connected layer generates the predicted token probabilities.

---

## 8. Training and Inference
### 8.1. Training
During training, the Transformer processes entire sequences in parallel. The model is optimized using **cross-entropy loss** and **teacher forcing**, where the correct output is provided as input at each step.

### 8.2. Inference Strategies
#### Greedy Decoding
- At each step, the word with the highest probability is selected.
- This method is fast but often leads to suboptimal results.

#### Beam Search
- At each step, the top `B` most probable words are selected.
- The model evaluates all possible next words and keeps the `B` most probable sequences.
- This method generally produces better translations.




      </section>

      <section id="references">
        <h3>References</h3>

        <div id="eyring2023unbalancedness" style="margin-bottom: 10px;">
          <p>[<span class="blue">Vaswani et al. 2017</span>] Attention Is All You Need</p>
        </div>

        <div id="annotated-diffusion" style="margin-bottom: 10px;">
          <p>[<span class="blue">The Illustrated Transformer</span>] https://jalammar.github.io/illustrated-transformer/</p>
        </div>

<div id="annotated-diffusion" style="margin-bottom: 10px;">
          <p>[<span class="blue">Transformer from scratch</span>] https://github.com/hkproj/transformer-from-scratch-notes/blob/main/Diagrams_V2.pdf</p>
        </div>
        
      </section>
    </div>
  </div>

  <script src="../assets/wowchemy.min.e8ee06ba8371980ffde659871dd593b0.js"></script>
  <script>
    var coll = document.getElementsByClassName("collapsible");
    var i;

    for (i = 0; i < coll.length; i++) {
      coll[i].addEventListener("click", function() {
        this.classList.toggle("active");
        var content = this.nextElementSibling;
        if (content.style.display === "block") {
          content.style.display = "none";
        } else {
          content.style.display = "block";
        }
      });
    }
  </script>
</body>
</html>